{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from collections import Counter\n",
    "from transformers import ViltConfig, AutoImageProcessor, ViTModel,  BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.datasets import make_classification\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from vqa import VQA\n",
    "import random\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:  <class 'dict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'image_id': 15704,\n",
       " 'question': \"Does the woman's dentist recommend this dessert?\",\n",
       " 'multiple_choices': ['deer and squirrel',\n",
       "  'ground',\n",
       "  'closet',\n",
       "  'sitting',\n",
       "  'being carried',\n",
       "  '2',\n",
       "  'red',\n",
       "  'blue',\n",
       "  'office supplies',\n",
       "  'oval',\n",
       "  '3',\n",
       "  '1',\n",
       "  '4',\n",
       "  'no',\n",
       "  'yes',\n",
       "  'white',\n",
       "  'yellow',\n",
       "  'floor'],\n",
       " 'question_id': 157040}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('.\\datasets\\Questions_Train_abstract_v002\\MultipleChoice_abstract_v002_train2015_questions.json') as json_file:\n",
    "    data_questions = json.load(json_file)\n",
    "\n",
    "    # print the type of data variable\n",
    "    print(\"Type: \", type(data_questions))\n",
    "\n",
    "data_questions['questions'][48]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type:  <class 'dict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'how many': 5956,\n",
       "         'what color is the': 4184,\n",
       "         'is the': 3436,\n",
       "         'where is the': 2621,\n",
       "         'what': 2329,\n",
       "         'what is': 2114,\n",
       "         'are the': 1759,\n",
       "         'what is the': 1758,\n",
       "         'is there a': 1417,\n",
       "         'does the': 1406,\n",
       "         'none of the above': 1387,\n",
       "         'is the woman': 1341,\n",
       "         'is the man': 1273,\n",
       "         'what is on the': 1237,\n",
       "         'is it': 930,\n",
       "         'is the girl': 904,\n",
       "         'is the boy': 850,\n",
       "         'is the dog': 845,\n",
       "         'are they': 834,\n",
       "         'who is': 775,\n",
       "         'what kind of': 759,\n",
       "         'what color are the': 757,\n",
       "         'what is in the': 742,\n",
       "         'what is the man': 724,\n",
       "         'is there': 696,\n",
       "         'what is the woman': 684,\n",
       "         'what are the': 627,\n",
       "         'what is the boy': 597,\n",
       "         'are there': 577,\n",
       "         'what is the girl': 556,\n",
       "         'is this': 547,\n",
       "         'how': 533,\n",
       "         'which': 524,\n",
       "         'how many people are': 511,\n",
       "         'is the cat': 504,\n",
       "         'why is the': 462,\n",
       "         'are': 461,\n",
       "         'will the': 445,\n",
       "         'what type of': 435,\n",
       "         'what is the dog': 413,\n",
       "         'do': 410,\n",
       "         'does': 409,\n",
       "         'is she': 409,\n",
       "         'do the': 402,\n",
       "         'is': 372,\n",
       "         'is the baby': 359,\n",
       "         'are there any': 357,\n",
       "         'is the lady': 354,\n",
       "         'can': 343,\n",
       "         'what animal is': 341,\n",
       "         'why': 329,\n",
       "         'where are the': 314,\n",
       "         'is the sun': 303,\n",
       "         'what are they': 300,\n",
       "         'did the': 290,\n",
       "         'how many pillows': 279,\n",
       "         'what is the cat': 277,\n",
       "         'what is the lady': 274,\n",
       "         'how many clouds are': 260,\n",
       "         'is that': 257,\n",
       "         'is the little girl': 255,\n",
       "         'how many bushes': 254,\n",
       "         'is he': 254,\n",
       "         'are these': 253,\n",
       "         'how many trees are': 249,\n",
       "         'are the people': 239,\n",
       "         'what color': 232,\n",
       "         'is the young': 217,\n",
       "         'how many windows are': 209,\n",
       "         'is this a': 206,\n",
       "         'what is the little': 206,\n",
       "         'how many pictures': 202,\n",
       "         'is the tv': 201,\n",
       "         'how many animals are': 194,\n",
       "         'who': 193,\n",
       "         'how many plants are': 191,\n",
       "         'how many birds are': 188,\n",
       "         'what color is': 183,\n",
       "         'what is the baby': 179,\n",
       "         'is anyone': 178,\n",
       "         'is the old man': 168})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('./datasets/Annotations_Train_abstract_v002/abstract_v002_train2015_annotations.json') as json_file:\n",
    "    data_annotations = json.load(json_file)\n",
    "\n",
    "    # print the type of data variable\n",
    "    print(\"Type: \", type(data_annotations))\n",
    "\n",
    "question_types = []\n",
    "does = []\n",
    "for idx in range(len(data_annotations['annotations'])):\n",
    "    # if data_annotations['annotations'][idx]['question_type'] == 'does the':\n",
    "    #     does.append(data_annotations['annotations'][idx])\n",
    "    #     break\n",
    "    question_types.append(data_annotations['annotations'][idx]['question_type'])\n",
    "\n",
    "    \n",
    "Counter(question_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        try:\n",
    "            with Image.open(img_path) as img:\n",
    "                images.append(img.copy())\n",
    "        except (IOError, FileNotFoundError):\n",
    "            print(f\"Error opening {filename}\")\n",
    "    return images\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = r'.\\datasets\\images\\scene_img_abstract_v002_train2015\\abstract_v002_train2015_000000015704.png'\n",
    "# images = load_image(image)117791\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_tensor(img: str = \"pali.png\", img_size: int = 224):\n",
    "    # Load image\n",
    "    image = Image.open(img)\n",
    "\n",
    "    # Define a transforms to convert the image to a tensor and apply preprocessing\n",
    "    transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Lambda(lambda image: image.convert(\"RGB\")),\n",
    "            transforms.Resize((img_size, img_size)),  # Resize the image to 256x256\n",
    "            transforms.ToTensor(),  # Convert the image to a tensor,\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "            ),  # Normalize the pixel values\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # apply transforms to the image\n",
    "    x = transform(image)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call model and processor\n",
    "img_model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n",
    "input = img_to_tensor(image, img_size=224)\n",
    "# with torch.no_grad():   \n",
    "#     img_output = img_model(input)\n",
    "\n",
    "# img_output.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "text_model = BertModel.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'away , yes , blue , 1 , 2 , mouse , couch , no , yellow , it belongs , 4 , red , on chair , his friend is coming over , 3 , white , bench , chair'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\n",
    "\n",
    "for word in data_questions['questions'][2]['multiple_choices'][:-1]:\n",
    "    text += word + \" , \"\n",
    "text += data_questions['questions'][2]['multiple_choices'][-1]\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 60, 1024])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embedding = tokenizer(\n",
    "    text=f\"{data_questions['questions'][1]['question']} multiple choices: {text}\",\n",
    "    padding='max_length',\n",
    "    max_length=60,\n",
    "    return_tensors='pt'\n",
    "    )\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_output = text_model(**text_embedding)\n",
    "\n",
    "text_output.last_hidden_state.shape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultimodalModel(nn.Module):\n",
    "    def __init__(self,  img_dim, text_dim, seq_len_img, seq_len_text, output_dim):\n",
    "        super(MultimodalModel, self).__init__()\n",
    "        \n",
    "        # Reducing dimension of text to match image dimensions if needed\n",
    "        self.text_dim_reducer = nn.Linear(text_dim, img_dim)\n",
    "        self.adaptive_pool = nn.AdaptiveMaxPool1d(seq_len_text)\n",
    "        \n",
    "        # Changing the size of image embedding to match text sequence length\n",
    "        self.img_dim_matcher = nn.Linear(768, 1024)  # Assuming original img_dim is 768 and target is 1024\n",
    "        # MLP for classification or regression\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(img_dim * seq_len_text, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, img_embedding, text_embedding, labels):\n",
    "        # Adjust the image embedding dimensions\n",
    "        img_embedding_adjusted = self.img_dim_matcher(img_embedding)\n",
    "        # Apply adaptive pooling to match sequence length\n",
    "        img_embedding_pooled = self.adaptive_pool(img_embedding_adjusted.transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "        # Optionally reduce dimension of text embeddings if necessary\n",
    "        text_embedding_reduced = self.text_dim_reducer(text_embedding)\n",
    "        \n",
    "        # Combine embeddings and process through MLP\n",
    "        combined_embedding = img_embedding_pooled * text_embedding_reduced\n",
    "        output = self.mlp(combined_embedding)\n",
    "        \n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dim = 1024\n",
    "text_dim = 1024\n",
    "seq_len_image = 197  \n",
    "seq_len_text = 512\n",
    "output_dim = 18\n",
    "\n",
    "model = MultimodalModel(image_dim, text_dim, seq_len_image, seq_len_text, output_dim)\n",
    "img_model.to('cuda:0')\n",
    "text_model.to('cuda:0')\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, questions, annotations, tokenizer, img_transform, img_dir):\n",
    "        self.questions = questions\n",
    "        self.annotations = annotations\n",
    "        self.tokenizer = tokenizer\n",
    "        self.img_transform = img_transform\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question_data = self.questions[idx]\n",
    "        annotation_data = self.annotations[idx]\n",
    "\n",
    "        text = \", \".join(question_data['multiple_choices'])\n",
    "        text_embedding = self.tokenizer(\n",
    "            text=f\"{question_data['question']} multiple choices: {text}\",\n",
    "            padding='max_length',\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        label = question_data['multiple_choices'].index(annotation_data['multiple_choice_answer'])\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        image_num = question_data['image_id']\n",
    "        image_path = f'{self.img_dir}/abstract_v002_train2015_{image_num:012}.png'\n",
    "        image_input = self.img_transform(image_path)\n",
    "\n",
    "        return text_embedding, image_input, label\n",
    "\n",
    "\n",
    "\n",
    "def train(epoch, batch_size=32):\n",
    "    model.to('cuda:0')\n",
    "    best = 0\n",
    "    loss_list = []\n",
    "    f1_list = []\n",
    "    dev_loss_list = []\n",
    "    dev_f1_list = []\n",
    "\n",
    "    PATH = './trained_model'\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, eps=1e-8)\n",
    "\n",
    "    # Create training and validation datasets and dataloaders\n",
    "    train_dataset = CustomDataset(data_questions['questions'][:20000], data_annotations['annotations'][:20000], tokenizer, img_to_tensor, './datasets/images/scene_img_abstract_v002_train2015')\n",
    "    dev_dataset = CustomDataset(data_questions['questions'][20000:22000], data_annotations['annotations'][20000:22000], tokenizer, img_to_tensor, './datasets/images/scene_img_abstract_v002_train2015')\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for epoch_num in range(epoch):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_f1 = 0\n",
    "\n",
    "        train_pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Training Epoch {epoch_num+1}\")\n",
    "\n",
    "        for batch_idx, (text_embeddings, image_inputs, labels) in train_pbar:\n",
    "            model.zero_grad()\n",
    "\n",
    "            text_embeddings = {k: v.squeeze(1).to('cuda:0') for k, v in text_embeddings.items()}\n",
    "            labels = labels.to('cuda:0')\n",
    "            image_inputs = image_inputs.to('cuda:0')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                text_output = text_model(**text_embeddings).last_hidden_state.to('cuda:0')\n",
    "                img_output = img_model(image_inputs).last_hidden_state.to('cuda:0')\n",
    "\n",
    "            output = model(img_output, text_output, labels).to('cuda:0')\n",
    "            loss_cf = nn.CrossEntropyLoss()\n",
    "            loss = loss_cf(output, labels)\n",
    "            f1 = f1_score(labels.cpu().numpy(), torch.argmax(F.softmax(output, dim=1).to('cuda:0'), dim=1).cpu().numpy(), average='macro', zero_division=0)\n",
    "            total_loss += loss.item()\n",
    "            total_f1 += f1\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_pbar.set_postfix({\"loss\": loss.item(), \"f1\": f1})\n",
    "\n",
    "        avg_f1 = total_f1 / len(train_loader)\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch: {epoch_num+1}, Loss: {avg_loss}, f1: {avg_f1}\")\n",
    "        loss_list.append(avg_loss)\n",
    "        f1_list.append(avg_f1)\n",
    "\n",
    "        model.eval()\n",
    "        total_dev_loss = 0\n",
    "        total_dev_f1 = 0\n",
    "\n",
    "        dev_pbar = tqdm(enumerate(dev_loader), total=len(dev_loader), desc=f\"Validation Epoch {epoch_num+1}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (text_embeddings, image_inputs, labels) in dev_pbar:\n",
    "                text_embeddings = {k: v.squeeze(1).to('cuda:0') for k, v in text_embeddings.items()}\n",
    "                labels = labels.to('cuda:0')\n",
    "                image_inputs = image_inputs.to('cuda:0')\n",
    "\n",
    "                text_output = text_model(**text_embeddings).last_hidden_state.to('cuda:0')\n",
    "                img_output = img_model(image_inputs).last_hidden_state.to('cuda:0')\n",
    "\n",
    "                output = model(img_output, text_output, labels).to('cuda:0')\n",
    "                loss = loss_cf(output, labels)\n",
    "                f1 = f1_score(labels.cpu().numpy(), torch.argmax(F.softmax(output, dim=1).to('cuda:0'), dim=1).cpu().numpy(), average='macro', zero_division=0)\n",
    "                total_dev_loss += loss.item()\n",
    "                total_dev_f1 += f1\n",
    "\n",
    "                dev_pbar.set_postfix({\"loss\": loss.item(), \"f1\": f1})\n",
    "\n",
    "        dev_f1 = total_dev_f1 / len(dev_loader)\n",
    "        dev_loss = total_dev_loss / len(dev_loader)\n",
    "        dev_loss_list.append(dev_loss)\n",
    "        dev_f1_list.append(dev_f1)\n",
    "\n",
    "        print(f\"Epoch: {epoch_num+1}, dev loss: {dev_loss}, dev f1: {dev_f1}\")\n",
    "        if dev_f1 > best:\n",
    "            best = dev_f1\n",
    "            print('Save model....')\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "                \n",
    "    for i in range(len(loss_list)):\n",
    "        print(f'epoch {i+1}: \\n\\ttrain_loss: {loss_list[i]}, train_f1:{f1_list[i]}\\n\\tdev_loss: {dev_loss_list[i]}, dev_f1:{dev_f1_list[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1:   0%|          | 1/625 [00:09<1:42:37,  9.87s/it, loss=2.92, f1=0.0131]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 91\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epoch, batch_size)\u001b[0m\n\u001b[0;32m     88\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     89\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 91\u001b[0m     train_pbar\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m: f1})\n\u001b[0;32m     93\u001b[0m avg_f1 \u001b[38;5;241m=\u001b[39m total_f1 \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[0;32m     94\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/63 [00:15<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m     test_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(test_loader)\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, f1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_f1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 45\u001b[0m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 30\u001b[0m, in \u001b[0;36mtest\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m image_inputs \u001b[38;5;241m=\u001b[39m image_inputs\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     29\u001b[0m text_output \u001b[38;5;241m=\u001b[39m text_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtext_embeddings)\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m img_output \u001b[38;5;241m=\u001b[39m \u001b[43mimg_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_inputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     32\u001b[0m output \u001b[38;5;241m=\u001b[39m test_model(img_output, text_output, labels)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     33\u001b[0m softmax \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSoftmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:577\u001b[0m, in \u001b[0;36mViTModel.forward\u001b[1;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[0;32m    571\u001b[0m     pixel_values \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mto(expected_dtype)\n\u001b[0;32m    573\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    574\u001b[0m     pixel_values, bool_masked_pos\u001b[38;5;241m=\u001b[39mbool_masked_pos, interpolate_pos_encoding\u001b[38;5;241m=\u001b[39minterpolate_pos_encoding\n\u001b[0;32m    575\u001b[0m )\n\u001b[1;32m--> 577\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    578\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    580\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    581\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    584\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    585\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm(sequence_output)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:407\u001b[0m, in \u001b[0;36mViTEncoder.forward\u001b[1;34m(self, hidden_states, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    400\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    401\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    402\u001b[0m         hidden_states,\n\u001b[0;32m    403\u001b[0m         layer_head_mask,\n\u001b[0;32m    404\u001b[0m         output_attentions,\n\u001b[0;32m    405\u001b[0m     )\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 407\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    409\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:364\u001b[0m, in \u001b[0;36mViTLayer.forward\u001b[1;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    361\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m attention_output \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[0;32m    363\u001b[0m \u001b[38;5;66;03m# in ViT, layernorm is also applied after self-attention\u001b[39;00m\n\u001b[1;32m--> 364\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayernorm_after\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    365\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(layer_output)\n\u001b[0;32m    367\u001b[0m \u001b[38;5;66;03m# second residual connection is done here\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\modules\\normalization.py:201\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:2546\u001b[0m, in \u001b[0;36mlayer_norm\u001b[1;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[0;32m   2542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[0;32m   2543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   2544\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[0;32m   2545\u001b[0m     )\n\u001b[1;32m-> 2546\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "image_dim = 1024\n",
    "text_dim = 1024\n",
    "seq_len_image = 197  \n",
    "seq_len_text = 512\n",
    "output_dim = 18\n",
    "\n",
    "def test():\n",
    "    test_model = MultimodalModel(image_dim, text_dim, seq_len_image, seq_len_text, output_dim)\n",
    "    test_model.load_state_dict(torch.load('./trained_model'))\n",
    "    test_model.to('cuda:0')\n",
    "\n",
    "    test_dataset = CustomDataset(data_questions['questions'][40000:42000], data_annotations['annotations'][40000:42000], tokenizer, img_to_tensor, './datasets/images/scene_img_abstract_v002_train2015')\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    total_loss = 0\n",
    "    total_f1 = 0\n",
    "    loss_cf = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_pbar = tqdm(enumerate(test_loader), total=len(test_loader), desc=\"Testing\")\n",
    "\n",
    "        for batch_idx, (text_embeddings, image_inputs, labels) in test_pbar:\n",
    "            test_model.eval()\n",
    "\n",
    "            text_embeddings = {k: v.squeeze(1).to('cuda:0') for k, v in text_embeddings.items()}\n",
    "            labels = labels.to('cuda:0')\n",
    "            image_inputs = image_inputs.to('cuda:0')\n",
    "\n",
    "            text_output = text_model(**text_embeddings).last_hidden_state.to('cuda:0')\n",
    "            img_output = img_model(image_inputs).last_hidden_state.to('cuda:0')\n",
    "\n",
    "            output = test_model(img_output, text_output, labels).to('cuda:0')\n",
    "            softmax = nn.Softmax(dim=1)\n",
    "            loss = loss_cf(output, labels)\n",
    "            f1 = f1_score(labels.cpu().numpy(), torch.argmax(softmax(output).to('cuda:0'), dim=1).cpu().numpy(), average='macro', zero_division=0)\n",
    "            total_loss += loss.item()\n",
    "            total_f1 += f1\n",
    "\n",
    "            test_pbar.set_postfix({\"loss\": loss.item(), \"f1\": f1})\n",
    "\n",
    "    test_f1 = total_f1 / len(test_loader)\n",
    "    test_loss = total_loss / len(test_loader)\n",
    "    print(f\"Loss: {test_loss}, f1: {test_f1}\")\n",
    "\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
